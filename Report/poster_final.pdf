\documentclass[landscape,a3,final,24pt]{issposter}

% If you encounter errors like "No room for a new ...", use etex (needs
% to be the first package. This happens, if too many registers are used
% because of too many packages or too many complex packages (like e.g. 
% tikz, pstricks and longtable). etex extends the number of registers.
% etex has to be the first package to be loaded!
% \usepackage{etex}

\usepackage[latin1]{inputenc}
\usepackage{multicol}
\usepackage{graphicx}
\usepackage[ngerman]{babel}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}

% If you want to use latex, dvips and ps2pdf, use hyperref to get correct
% page sizes. With hyperref, you won't have to specify them on the command
% line.
% \usepackage[pdfpagemode=UseNone,dvips,pdfborder={0 0 0}]{hyperref}

% \usepackage{tikz} % nice graphics, also used for the fancy boxes

% \usepackage{pstricks,pst-plot,pst-node} % nice graphics with PS
% \psset{unit=3mm,nodesep=1.5,linewidth=.3}
% Use auto-pst-pdf, to use PSTricks with pdflatex. In this case,
% you have to add --shell-escape to your pdflatex command line.
% \usepackage{auto-pst-pdf}

% Set the font sizes. Change these, if you want to change only one or two.
% Use the font size options of the document class (14pt, 17pt, 20pt, 15pt, 30pt, 36pt),
% if you want to change all the sizes at once.
\renewcommand{\titlesize}{\Huge}
\renewcommand{\sectionsize} {\Large}
\renewcommand{\authorsize}{\Large}
\renewcommand{\instsize}{\large}

% Insert your data
\title{\MakeUppercase{Diabetic Retinopathy Detection, Team 2}}
\author{Xu Siying, Li Liangzhu}
\institute{Institute of Signal Processing and System Theory, University of Stuttgart, Germany}
\conference{\normalsize \textbf{Deep Learning Lab 2020}, February 4, Stuttgart, Germany }

% -----------------------------------------------------------------------------
\begin{document}
\maketitle

% Justified text sometimes looks strange in narrow columns
\raggedright

\begin{multicols}{3}
\section{Input pipeline}
\begin{itemize}
    \item Build datasets
    \begin{itemize}
        \item Read data: Images are first read by file names for later preprocessing. Labels are read and saved in one-hot coding form.
        \item Shuffle data index: Since the array of filenames has lower dimension than the array of images has, we shuffle the index of filenames, thereby reducing the running time of the preprocessing stage.
        \item Parse function: Firstly, it will transform a pair of input data (filename, label) to the corresponding data pair (image, label). Secondly, images will be decoded, resized and normalized and then transformed by one data augmentation method based on the random number it receives.
        \item Datasets: including datasets of images and labels. The dataset is split into 383 samples for training and 30 samples for validation.
        The test dataset contains 103 samples and will not be shuffled.
    \end{itemize}
    \item Online data augmentation:The data augmentation function is called during training to obtain infinite random data.   
\end{itemize}


\section{Model Architecture}
\begin{itemize}
	\item Self defined model
	\begin{itemize}
		\item Model architecture: The model has totally 19 layers, including in total 5 convolutional layers, 5 max pooling layers, 5 dropout layers, 3 dense layers and 1 flatten layer.
		\item Performance: To prevent overfitting, the training is early stopped when training iterations reach 900 times. The test accuracy can reach 0.76.
	\end{itemize}
	\item Transfer learning
    \begin{itemize}
        \item Model architecture: Based on the Xception model, two dense layers are added after removing its original last dense layer.
        \item Layers: The Xception model has a total of 100 layers. The first 100 layers are set as untrainable and the next 33 layers are set as fine-tuning layers.
        \item Performance: After about 150 training steps, the training accuracy can reach 0.99, the validation accuracy is about 0.9, and the test accuracy can reach 0.825. The performance has been significantly improved compared to the self defined model.
    \end{itemize}
\end{itemize}

\section{Training and Evaluation}
\begin{itemize}
	\item Training routine
	\begin{itemize}
		\item Self defined model: we have written our own training routine to train the self defined model instead of using fit method, the optimizer is Adam with learning rate of 0.001. The training iteration steps are set to be 1600, but the real iteration numbers will be decided by the validation accuracy and loss, in order to prevent overfitting.
		\item Transfer learning model: use fit method to train the transfer learning model. The optimizers is also Adam. The learning rate is 1/20 of self defined model.
	\end{itemize}
	\item Evaluation
	\begin{itemize}
		\item Deep visualization:
		\begin{itemize}
		    \item Method: Gradient-weighted Class Activation Mapping, which can highlight the important regions in an image for a specific prediction.
		    \item Performance:
		\end{itemize}
	\begin{figure}
		\includegraphics[scale=0.4]{gradcam.png}
		\caption{Deep visualization by Grad-CAM}
	\end{figure}
		\item Tensorboard: By using tensorboard, we track and visualize accuracy and loss, which helps us analyze the performance of model. It is applied both on self defined model and transfer learning model.
		\item Checkpoint: check points are saved every 10 steps, which saves all parameters of the model.
		\item Confusion matrix: through the confusion matrix, we can intuitively compare the correctness of binary classification of different models and methods.
	\end{itemize}
    \begin{figure}
    	\includegraphics[scale=0.2]{withaug.png}\includegraphics[scale=0.2]{withoutaug}
    	\caption{Confusion matrix (a) with data augmentation (b) without data augmentation}
    \end{figure}
\end{itemize}

\end{multicols}
\end{document}


