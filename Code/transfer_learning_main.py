# -*- coding: utf-8 -*-
"""transfer_learning_main.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1H262-dZC6KHeIb76leFsnxd1evKs1nkN
"""

!pip3 install tensorflow-gpu==2.0.0-beta0
!pip install tensorflow-addons

import tensorflow as tf
import os
import glob
import numpy as np
import numpy.random as npr
import pandas as pd
import tensorflow.keras as k
import tensorflow.keras.layers as l
import matplotlib.pyplot as plt
import tensorflow_addons as tfa
import random
import cv2
import tensorboard
import datetime

from tensorflow.keras.layers import Dense, Flatten, Conv2D
from tensorflow.keras import Model
from keras import optimizers
from tensorboard import notebook
from sklearn.metrics import confusion_matrix

from google.colab import drive
drive.mount('/content/gdrive')

buffersize = 50
batchsize = 32
N_samples = 413
N_trainingsamples = 383
N_validationsamples = 30
N_testsamples = 103
learning_rate = 0.001

# build dataset of filenames(training set)

name_path = "/content/gdrive/My Drive/Master Lab Course/Datasets/B. Disease Grading/1. Original Images/a. Training Set"
filenames = []
file_dir = os.listdir(name_path)

for filename in sorted(file_dir):
    filenames.append(filename)

filename_ds = tf.data.Dataset.from_tensor_slices(sorted(filenames))

# data augmentation

@tf.function
def flip1(img):
   img_flipped = tf.image.random_flip_up_down(img)
   img = tf.cast(img_flipped, tf.float32) / 255.0
   return img

def flip2(img):
   img_flipped = tf.image.random_flip_left_right(img)
   img = tf.cast(img_flipped, tf.float32) / 255.0
   return img

def color(img):
    img = tf.image.random_hue(img, 0.02)
    img = tf.image.random_brightness(img, 0.02)
    img = tf.cast(img, tf.float32) / 255.0
    return img
    
def rotate(img):
    angles = tf.random.uniform([], minval=0, maxval=359, dtype=tf.dtypes.float32)
    img = tfa.image.rotate(img, angles, interpolation='NEAREST', name=None)
    img = tf.cast(img, tf.float32) / 255.0
    return img

def zoom(img):
    scales = list(np.arange(0.8, 1.0, 0.01))
    boxes = np.zeros((len(scales), 2))
    for i, scale in enumerate(scales):
        x1 = 0.5 - (0.5 * scale)
        y1 = 0.5 + (0.5 * scale)
        boxes[i] = [x1, y1]

    # Create different crops for an image
    img = tf.image.crop_and_resize(img, boxes=boxes, box_indices=np.zeros(len(scales)), crop_size=(256, 256))
    crops = tf.image.crop_and_resize([img], boxes=boxes, box_indices=np.zeros(len(scales)),
                                             crop_size=(256, 256))
    # Return a random crop
    return crops[tf.random.uniform(shape=[], minval=0, maxval=len(scales), dtype=tf.int32)]

def rot90(img):
    img = tf.image.rot90(img, tf.random.uniform(shape=[], minval=0, maxval=4, dtype=tf.int32))
    img = tf.cast(img, tf.float32) / 255.0
    return img

def random_jpg_quality(img):  
    a = random.randint(1,100)
    b = random.randint(1,100)
    if a<b:
      min = a
      max = b
    elif a == b:
      min = a-2
      max = a
    else:
      min = b
      max = a
    img = tf.image.random_jpeg_quality(
                          img,
                          min_jpeg_quality = min,
                          max_jpeg_quality = max,
                          seed=None)
    img = tf.cast(img, tf.float32) / 255.0
    return img

# build dataset of labels(training and validation)
N_prefetch = 8 
N_parallel_iteration = 4
N_testsamples = 103
csv_path = "/content/gdrive/My Drive/Master Lab Course/Datasets/B. Disease Grading/2. Groundtruths/a. IDRiD_Disease Grading_Training Labels.csv"
files_csv = pd.read_csv(csv_path, usecols=[1])
labels = np.zeros(shape=(N_samples,))
csv_tensor = tf.convert_to_tensor(files_csv.values, dtype=tf.int32)
csv_tensor = tf.map_fn(lambda x: 1 if x > 1 else 0, csv_tensor)
for i in range(N_samples):
    if csv_tensor[i] == 1:
         labels[i] = 1
    else:
         labels[i] = 0

# build dataset of images(Training and validation set)

def load_file_names():
  files = glob.glob("/content/gdrive/My Drive/Master Lab Course/Datasets/B. Disease Grading/1. Original Images/a. Training Set/*.jpg")
  return files

img_list = []
files = sorted(load_file_names())

def parse_train_function(files,labels):
    image_string = tf.io.read_file(files)
    image_decoded = tf.io.decode_jpeg(image_string)
    image_resized = tf.image.resize_with_pad(image_decoded,256,256)
 
    a = random.randint(1,12)
    if a == 1:
      img = zoom(image_resized)
      img = tf.cast(img, tf.float32) / 255.0  
    elif a== 2:
      img = rotate(image_resized)
    elif a== 3:
      img = rot90(image_resized)
    elif a== 4:
      img = color(image_resized)    
    elif a== 5:
      img = flip1(image_resized)
    elif a== 6:
      img = flip2(image_resized)
    elif a== 7:
      img = random_jpg_quality(image_resized)    
    else:
      img = image_resized  
      img = tf.cast(img, tf.float32) / 255.0       
    label = labels
    return img, label

def parse_val_function(files,labels):
    image_string = tf.io.read_file(files)
    image_decoded = tf.io.decode_jpeg(image_string)
    image_resized = tf.image.resize_with_pad(image_decoded,256,256)
    img = tf.cast(image_resized, tf.float32) / 255.0
    label = labels
    return img, label

def build_train_ds(files, labels, batchsize):
    ds = tf.data.Dataset.from_tensor_slices((files, labels))
    ds = ds.map(parse_train_function, N_parallel_iteration)
    ds = ds.shuffle(380).batch(batchsize).repeat(-1).prefetch(N_prefetch)
    return ds

def build_val_ds(files, labels, batchsize):
    ds = tf.data.Dataset.from_tensor_slices((files, labels))
    ds = ds.map(parse_val_function, N_parallel_iteration)
    ds = ds.shuffle(20).batch(20).prefetch(N_prefetch)
    return ds

#build total dataset(training set and validtion set)
shuffle_idx = np.arange(0, N_samples)
np.random.shuffle(shuffle_idx)
files = [files[i] for i in shuffle_idx]
labels = [labels[i] for i in shuffle_idx]

train_ds = build_train_ds(files[0:N_trainingsamples], labels[0:N_trainingsamples], batchsize)
val_ds = build_val_ds(files[N_trainingsamples:N_samples], labels[N_trainingsamples:N_samples], batchsize)

"""
# show image
img = plt.imread(files[100])
plt.imshow(img)
plt.title(labels[100])
plt.show()"""

# build dataset of images(testing set)

def load_testfile_names():
    files = glob.glob("/content/gdrive/My Drive/Master Lab Course/Datasets/B. Disease Grading/1. Original Images/b. Testing Set/*.jpg")
    return files

img_list_test = []
test_files = load_testfile_names()

for file in sorted(test_files):
    image_string = tf.io.read_file(file)
    image_decoded = tf.io.decode_image(image_string)
    image_resized = tf.image.resize_with_pad(image_decoded,256,256)
    img = tf.cast(image_resized, tf.float32) / 255.0
    img_list_test.append(img)
    
img_tensor_test = tf.convert_to_tensor(img_list_test, dtype=tf.float32)
img_test_ds = tf.data.Dataset.from_tensor_slices(img_tensor_test)

# build dataset of labels(testing set)

csv_path_test = "/content/gdrive/My Drive/Master Lab Course/Datasets/B. Disease Grading/2. Groundtruths/b. IDRiD_Disease Grading_Testing Labels.csv"
files_csv_test = pd.read_csv(csv_path_test, usecols=[1])
t = np.zeros(shape=(N_testsamples,))
csv_tensor = tf.convert_to_tensor(files_csv_test.values, dtype=tf.float32)
csv_tensor = tf.map_fn(lambda x: 1 if x > 1 else 0, csv_tensor)
for i in range(N_testsamples):
    if csv_tensor[i] == 1:
         t[i] = 1
    else:
         t[i] = 0

labels_ds_test = tf.data.Dataset.from_tensor_slices(t)

# build total dataset(testing set)
test_ds = tf.data.Dataset.zip((img_test_ds, labels_ds_test)).batch(batchsize)

# set up tensorboard

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

from datetime import datetime
from packaging import version

import functools
import tensorflow as tf
import tensorflow_datasets as tfds
from tensorflow.python.keras import backend
from tensorflow.python.keras import layers

import numpy as np

print("TensorFlow version: ", tf.__version__)
assert version.parse(tf.__version__).release[0] >= 2, \
    "This notebook requires TensorFlow 2.0 or above."

# Confirm TensorFlow can see the GPU.

device_name = tf.test.gpu_device_name()
if not tf.test.is_gpu_available():
  raise SystemError('GPU device not found')
print('Found GPU at: {}'.format(device_name))

# profiling
log_dir="logs/profile/" + datetime.now().strftime("%Y%m%d-%H%M%S")
tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1, profile_batch = 3)

# create log data to see accuracy
logdir = "./log"
tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)

# load base model
conv_base = k.applications.xception.Xception(weights='imagenet',
                                             include_top=False,     # excluding the last dense layer
                                             input_shape=(256,256,3),
                                             pooling='avg')

# add 2 dense layers
model = k.Sequential()
model.add(conv_base)
model.add(l.Dense(512,activation='relu'))
model.add(l.Dense(1,activation='sigmoid'))

model.summary()

# the first 100 layers: set as untrainable
# the last 33 layers: set as fine tuning layers

conv_base.train = True
fine_tune_at = -33

for layer in conv_base.layers[:fine_tune_at]:
  l.trainable = False

# compile the model
model.compile(loss='binary_crossentropy',
              optimizer=tf.keras.optimizers.Adam(lr=0.0005/10),#lr用原来的十分之一
              metrics=['accuracy'])

epochs = 5
fine_tune_epochs = 5 
total_epochs = epochs + fine_tune_epochs

history = model.fit(   
    train_ds,
    validation_data = val_ds,
    shuffle = True,
    steps_per_epoch = 30,
    validation_steps = 2,
    epochs = total_epochs,
    callbacks=[tensorboard_callback]
)

# plot the training and validation accuracy

import matplotlib.pyplot as plt

plt.plot(history.history['accuracy'], label='accuracy')
plt.plot(history.history['val_accuracy'], label = 'val_accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.ylim([0.5, 1])
plt.legend(loc='lower right')

# calculate test accuracy

test_loss, test_acc = model.evaluate(test_ds, verbose=2)
print(test_acc)

!tar -zcvf logs.tar.gz logs/profile/

# condusion matrix

from sklearn.metrics import confusion_matrix

def plot_confusion_matrix(cm, savename, title='Confusion Matrix'):
    plt.figure(figsize=(12, 8), dpi=100)
    np.set_printoptions(precision=2)

    # the probabilities in confusion matrix
    ind_array = np.arange(len(classes))
    x, y = np.meshgrid(ind_array, ind_array)
    for x_val, y_val in zip(x.flatten(), y.flatten()):
        c = cm[y_val][x_val]
        if c > 0.001:
            plt.text(x_val, y_val, "%0.2f" % (c,), color='red', fontsize=15, va='center', ha='center')

    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.binary)
    plt.title(title)
    plt.colorbar()
    xlocations = np.array(range(len(classes)))
    plt.xticks(xlocations, classes, rotation=90)
    plt.yticks(xlocations, classes)
    plt.ylabel('Actual label')
    plt.xlabel('Predict label')

    # offset the tick
    tick_marks = np.array(range(len(classes))) + 0.5
    plt.gca().set_xticks(tick_marks, minor=True)
    plt.gca().set_yticks(tick_marks, minor=True)
    plt.gca().xaxis.set_ticks_position('none')
    plt.gca().yaxis.set_ticks_position('none')
    plt.grid(True, which='minor', linestyle='-')
    plt.gcf().subplots_adjust(bottom=0.15)

    # show confusion matrix
    plt.savefig(savename, format='png')
    plt.show()


classes = ['referable','non-referable']

true_arr = []
pred_arr = []
for i,l in test_ds:
   pred = model(i)
   pred_arr.append(pred)
   true_arr.append(l)
   # Update val metrics

p_com1 = np.hstack((np.transpose(pred_arr[0]),np.transpose(pred_arr[1])))

p_com2 = np.hstack((np.transpose(pred_arr[2]),np.transpose(pred_arr[3])))
y_pred = np.hstack((p_com1,p_com2))
y_pred = np.reshape(y_pred.round(), (103,))

l_com1 = np.hstack((true_arr[0],true_arr[1]))
l_com2 = np.hstack((true_arr[2],true_arr[3]))
y_true = np.hstack((l_com1,l_com2))
print("y_pred: ")
print(y_pred)
print("y_true: ")
print(y_true)


# plot confusion matrix
cm = confusion_matrix(y_true, y_pred)

plot_confusion_matrix(cm, 'confusion_matrix.png', title='confusion matrix')

# grad cam implementation

def grad_cam(model, image, class_index, layer_name):
  """
    Args:
       model: model
       image: image input
       class_index: class index
       layer_name: last convolution layer name
    """
  
  grad_model = tf.keras.models.Model([model.inputs], [model.get_layer(layer_name).output, model.output])

  # obtain class output
  class_output = model.output[:,class_index]

  # obtain the output and loss of the layer which we want
  with tf.GradientTape() as tape:
    convolution_output, predictions = grad_model(image)
    loss = predictions[:, class_index]
    
  output = convolution_output[0]

  # calculate the gradients
  grads = tape.gradient(loss, convolution_output)[0]


  #get average weights
  weights = np.mean(grads, axis=(0, 1))

  # class activation mapping
  cam = np.ones(output.shape[0:2], dtype=np.float32)
  for i, w in enumerate(weights):
    cam += w * output[:, :, i]

  # express the gradient weight in RGB
  cam = cv2.resize(cam.numpy(), (256, 256))
  cam = np.maximum(cam, 0)
  heatmap = (cam - cam.min()) / (cam.max() - cam.min())
  cam = cv2.applyColorMap(np.uint8(255*heatmap), cv2.COLORMAP_JET)

  #plot grad cam visualization
  fig=plt.figure(figsize=(15, 5))
  fig.add_subplot(1, 2, 1)
  plt.title("Original Image")
  plt.imshow(img)
  fig.add_subplot(1, 2, 2)
  plt.title("Deep Visualization")
  plt.imshow(cam)
  plt.show()

# visualize test dataset

test_ds_unbatch = test_ds.unbatch()

for i, l in test_ds_unbatch:
  img = i
  image = tf.expand_dims(i, 0)
  label = np.argmax(l,axis=0)
  pred = model.predict(image)
  grad_cam(model, image, label, "block14_sepconv2_bn")

!pip uninstall tb-nightly tensorboard tensorflow tensorflow-estimator tensorflow-gpu tf-estimator-nightly
!pip install tensorflow-gpu
!pip3 install --upgrade grpcio

!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip
!unzip ngrok-stable-linux-amd64.zip

LOG_DIR = './log'
get_ipython().system_raw(
    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'
    .format(LOG_DIR)
)
 
get_ipython().system_raw('./ngrok http 6006 &')
 
! curl -s http://localhost:4040/api/tunnels | python3 -c \
    "import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])"