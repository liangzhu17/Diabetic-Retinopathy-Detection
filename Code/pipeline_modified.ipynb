{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pipeline_modified.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ObJCuZQXBriZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 557
        },
        "outputId": "db077df4-9335-45af-cedd-7d87bb32dca4"
      },
      "source": [
        "!pip3 install tensorflow-gpu==2.0.0-beta0"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-gpu==2.0.0-beta0\n",
            "  Using cached https://files.pythonhosted.org/packages/e8/7e/87c4c94686cda7066f52cbca4c344248516490acdd6b258ec6b8a805d956/tensorflow_gpu-2.0.0b0-cp36-cp36m-manylinux1_x86_64.whl\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta0) (0.34.2)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta0) (1.12.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta0) (1.15.0)\n",
            "Requirement already satisfied: tf-estimator-nightly<1.14.0.dev2019060502,>=1.14.0.dev2019060501 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta0) (1.14.0.dev2019060501)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta0) (1.11.2)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta0) (0.8.1)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta0) (0.9.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta0) (0.2.2)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta0) (1.17.5)\n",
            "Requirement already satisfied: tb-nightly<1.14.0a20190604,>=1.14.0a20190603 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta0) (1.14.0a20190603)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta0) (1.1.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta0) (0.1.8)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta0) (3.10.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta0) (1.0.8)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta0) (1.1.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190604,>=1.14.0a20190603->tensorflow-gpu==2.0.0-beta0) (45.1.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190604,>=1.14.0a20190603->tensorflow-gpu==2.0.0-beta0) (0.16.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190604,>=1.14.0a20190603->tensorflow-gpu==2.0.0-beta0) (3.1.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow-gpu==2.0.0-beta0) (2.8.0)\n",
            "Installing collected packages: tensorflow-gpu\n",
            "  Found existing installation: tensorflow-gpu 2.1.0\n",
            "    Uninstalling tensorflow-gpu-2.1.0:\n",
            "      Successfully uninstalled tensorflow-gpu-2.1.0\n",
            "Successfully installed tensorflow-gpu-2.0.0b0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tensorflow"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZQvOBSrBvew",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "613d1801-af0c-46ad-d40c-e25ba2941758"
      },
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import numpy.random as npr\n",
        "import pandas as pd\n",
        "import tensorflow.keras as k\n",
        "import tensorflow.keras.layers as l\n",
        "import matplotlib.pyplot as plt\n",
        "# import tensorflow_addons as tfa\n",
        "import random\n",
        "import cv2\n",
        "import tensorboard\n",
        "import datetime\n",
        "\n",
        "from tensorflow.keras.layers import Dense, Flatten, Conv2D\n",
        "from tensorflow.keras import Model\n",
        "from keras import optimizers\n",
        "from tensorboard import notebook\n",
        "from sklearn.metrics import confusion_matrix"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xPvZ8__KBygf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        },
        "outputId": "4dd202b3-def3-4f2d-8137-dd10e1e6ae3c"
      },
      "source": [
        "\n",
        "# create connection with colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WGNdoixgI-hx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "b42f7fd1-b7f8-4d61-dfb6-35f41e7b6371"
      },
      "source": [
        "buffersize = 50\n",
        "batchsize = 30\n",
        "N_samples = 413\n",
        "N_trainingsamples = 383\n",
        "N_validationsamples = 30\n",
        "N_testsamples = 103\n",
        "learning_rate = 0.001\n",
        "\n",
        "\"\"\"get the filenames of images in training set\"\"\"\n",
        "name_path = \"/content/gdrive/My Drive/Masterlab/a. Training Set\"\n",
        "file_dir = os.listdir(name_path)\n",
        "\n",
        "\n",
        "\"\"\"check the order of the filenames\"\"\"\n",
        "print(sorted(file_dir))  \n",
        " \n",
        "\n",
        "N_prefetch = 8     # Define the number of data to be prefetched for training\n",
        "N_parallel_iteration = 4  \n",
        "\n",
        "\n",
        "# load the csv file for labels\n",
        "csv_path_train = \"/content/gdrive/My Drive/Masterlab/a. IDRiD_Disease Grading_Training Labels.csv\"\n",
        "files_csv_train = pd.read_csv(csv_path, usecols=[1])\n",
        "\n",
        "\n",
        "\"\"\"Create training labels and validation labels together in one-hot coding form, \n",
        "   they will be seperated later\"\"\"\n",
        "def create_label(csv, sample_num):\n",
        "  labels = np.zeros(shape=(sample_num, 2))   \n",
        "  csv_tensor = tf.convert_to_tensor(csv.values, dtype=tf.int32)\n",
        "  csv_tensor = tf.map_fn(lambda x: 1 if x > 1 else 0, csv.values)\n",
        "  for i in range(sample_num):\n",
        "\n",
        "    if csv_tensor[i] == 1:\n",
        "        labels[i][0] = 1\n",
        "    else:\n",
        "        labels[i][1] = 1\n",
        "\n",
        "  return labels\n",
        "\n",
        "\n",
        "labels = create_label(files_csv_train, N_samples)   \n",
        "\n",
        "\n",
        "# build dataset of images(Training and validation set)\n",
        "\n",
        "def load_file_names():\n",
        "    files = glob.glob(\"/content/gdrive/My Drive/Masterlab/a. Training Set/*.jpg\")\n",
        "    return files\n",
        "\n",
        "\n",
        "files = sorted(load_file_names())\n"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['IDRiD_001.jpg', 'IDRiD_002.jpg', 'IDRiD_003.jpg', 'IDRiD_004.jpg', 'IDRiD_005.jpg', 'IDRiD_006.jpg', 'IDRiD_007.jpg', 'IDRiD_008.jpg', 'IDRiD_009.jpg', 'IDRiD_010.jpg', 'IDRiD_011.jpg', 'IDRiD_012.jpg', 'IDRiD_013.jpg', 'IDRiD_014.jpg', 'IDRiD_015.jpg', 'IDRiD_016.jpg', 'IDRiD_017.jpg', 'IDRiD_018.jpg', 'IDRiD_019.jpg', 'IDRiD_020.jpg', 'IDRiD_021.jpg', 'IDRiD_022.jpg', 'IDRiD_023.jpg', 'IDRiD_024.jpg', 'IDRiD_025.jpg', 'IDRiD_026.jpg', 'IDRiD_027.jpg', 'IDRiD_028.jpg', 'IDRiD_029.jpg', 'IDRiD_030.jpg', 'IDRiD_031.jpg', 'IDRiD_032.jpg', 'IDRiD_033.jpg', 'IDRiD_034.jpg', 'IDRiD_035.jpg', 'IDRiD_036.jpg', 'IDRiD_037.jpg', 'IDRiD_038.jpg', 'IDRiD_039.jpg', 'IDRiD_040.jpg', 'IDRiD_041.jpg', 'IDRiD_042.jpg', 'IDRiD_043.jpg', 'IDRiD_044.jpg', 'IDRiD_045.jpg', 'IDRiD_046.jpg', 'IDRiD_047.jpg', 'IDRiD_048.jpg', 'IDRiD_049.jpg', 'IDRiD_050.jpg', 'IDRiD_051.jpg', 'IDRiD_052.jpg', 'IDRiD_053.jpg', 'IDRiD_054.jpg', 'IDRiD_055.jpg', 'IDRiD_056.jpg', 'IDRiD_057.jpg', 'IDRiD_058.jpg', 'IDRiD_059.jpg', 'IDRiD_060.jpg', 'IDRiD_061.jpg', 'IDRiD_062.jpg', 'IDRiD_063.jpg', 'IDRiD_064.jpg', 'IDRiD_065.jpg', 'IDRiD_066.jpg', 'IDRiD_067.jpg', 'IDRiD_068.jpg', 'IDRiD_069.jpg', 'IDRiD_070.jpg', 'IDRiD_071.jpg', 'IDRiD_072.jpg', 'IDRiD_073.jpg', 'IDRiD_074.jpg', 'IDRiD_075.jpg', 'IDRiD_076.jpg', 'IDRiD_077.jpg', 'IDRiD_078.jpg', 'IDRiD_079.jpg', 'IDRiD_080.jpg', 'IDRiD_081.jpg', 'IDRiD_082.jpg', 'IDRiD_083.jpg', 'IDRiD_084.jpg', 'IDRiD_085.jpg', 'IDRiD_086.jpg', 'IDRiD_087.jpg', 'IDRiD_088.jpg', 'IDRiD_089.jpg', 'IDRiD_090.jpg', 'IDRiD_091.jpg', 'IDRiD_092.jpg', 'IDRiD_093.jpg', 'IDRiD_094.jpg', 'IDRiD_095.jpg', 'IDRiD_096.jpg', 'IDRiD_097.jpg', 'IDRiD_098.jpg', 'IDRiD_099.jpg', 'IDRiD_100.jpg', 'IDRiD_101.jpg', 'IDRiD_102.jpg', 'IDRiD_103.jpg', 'IDRiD_104.jpg', 'IDRiD_105.jpg', 'IDRiD_106.jpg', 'IDRiD_107.jpg', 'IDRiD_108.jpg', 'IDRiD_109.jpg', 'IDRiD_110.jpg', 'IDRiD_111.jpg', 'IDRiD_112.jpg', 'IDRiD_113.jpg', 'IDRiD_114.jpg', 'IDRiD_115.jpg', 'IDRiD_116.jpg', 'IDRiD_117.jpg', 'IDRiD_118.jpg', 'IDRiD_119.jpg', 'IDRiD_120.jpg', 'IDRiD_121.jpg', 'IDRiD_122.jpg', 'IDRiD_123.jpg', 'IDRiD_124.jpg', 'IDRiD_125.jpg', 'IDRiD_126.jpg', 'IDRiD_127.jpg', 'IDRiD_128.jpg', 'IDRiD_129.jpg', 'IDRiD_130.jpg', 'IDRiD_131.jpg', 'IDRiD_132.jpg', 'IDRiD_133.jpg', 'IDRiD_134.jpg', 'IDRiD_135.jpg', 'IDRiD_136.jpg', 'IDRiD_137.jpg', 'IDRiD_138.jpg', 'IDRiD_139.jpg', 'IDRiD_140.jpg', 'IDRiD_141.jpg', 'IDRiD_142.jpg', 'IDRiD_143.jpg', 'IDRiD_144.jpg', 'IDRiD_145.jpg', 'IDRiD_146.jpg', 'IDRiD_147.jpg', 'IDRiD_148.jpg', 'IDRiD_149.jpg', 'IDRiD_150.jpg', 'IDRiD_151.jpg', 'IDRiD_152.jpg', 'IDRiD_153.jpg', 'IDRiD_154.jpg', 'IDRiD_155.jpg', 'IDRiD_156.jpg', 'IDRiD_157.jpg', 'IDRiD_158.jpg', 'IDRiD_159.jpg', 'IDRiD_160.jpg', 'IDRiD_161.jpg', 'IDRiD_162.jpg', 'IDRiD_163.jpg', 'IDRiD_164.jpg', 'IDRiD_165.jpg', 'IDRiD_166.jpg', 'IDRiD_167.jpg', 'IDRiD_168.jpg', 'IDRiD_169.jpg', 'IDRiD_170.jpg', 'IDRiD_171.jpg', 'IDRiD_172.jpg', 'IDRiD_173.jpg', 'IDRiD_174.jpg', 'IDRiD_175.jpg', 'IDRiD_176.jpg', 'IDRiD_177.jpg', 'IDRiD_178.jpg', 'IDRiD_179.jpg', 'IDRiD_180.jpg', 'IDRiD_181.jpg', 'IDRiD_182.jpg', 'IDRiD_183.jpg', 'IDRiD_184.jpg', 'IDRiD_185.jpg', 'IDRiD_186.jpg', 'IDRiD_187.jpg', 'IDRiD_188.jpg', 'IDRiD_189.jpg', 'IDRiD_190.jpg', 'IDRiD_191.jpg', 'IDRiD_192.jpg', 'IDRiD_193.jpg', 'IDRiD_194.jpg', 'IDRiD_195.jpg', 'IDRiD_196.jpg', 'IDRiD_197.jpg', 'IDRiD_198.jpg', 'IDRiD_199.jpg', 'IDRiD_200.jpg', 'IDRiD_201.jpg', 'IDRiD_202.jpg', 'IDRiD_203.jpg', 'IDRiD_204.jpg', 'IDRiD_205.jpg', 'IDRiD_206.jpg', 'IDRiD_207.jpg', 'IDRiD_208.jpg', 'IDRiD_209.jpg', 'IDRiD_210.jpg', 'IDRiD_211.jpg', 'IDRiD_212.jpg', 'IDRiD_213.jpg', 'IDRiD_214.jpg', 'IDRiD_215.jpg', 'IDRiD_216.jpg', 'IDRiD_217.jpg', 'IDRiD_218.jpg', 'IDRiD_219.jpg', 'IDRiD_220.jpg', 'IDRiD_221.jpg', 'IDRiD_222.jpg', 'IDRiD_223.jpg', 'IDRiD_224.jpg', 'IDRiD_225.jpg', 'IDRiD_226.jpg', 'IDRiD_227.jpg', 'IDRiD_228.jpg', 'IDRiD_229.jpg', 'IDRiD_230.jpg', 'IDRiD_231.jpg', 'IDRiD_232.jpg', 'IDRiD_233.jpg', 'IDRiD_234.jpg', 'IDRiD_235.jpg', 'IDRiD_236.jpg', 'IDRiD_237.jpg', 'IDRiD_238.jpg', 'IDRiD_239.jpg', 'IDRiD_240.jpg', 'IDRiD_241.jpg', 'IDRiD_242.jpg', 'IDRiD_243.jpg', 'IDRiD_244.jpg', 'IDRiD_245.jpg', 'IDRiD_246.jpg', 'IDRiD_247.jpg', 'IDRiD_248.jpg', 'IDRiD_249.jpg', 'IDRiD_250.jpg', 'IDRiD_251.jpg', 'IDRiD_252.jpg', 'IDRiD_253.jpg', 'IDRiD_254.jpg', 'IDRiD_255.jpg', 'IDRiD_256.jpg', 'IDRiD_257.jpg', 'IDRiD_258.jpg', 'IDRiD_259.jpg', 'IDRiD_260.jpg', 'IDRiD_261.jpg', 'IDRiD_262.jpg', 'IDRiD_263.jpg', 'IDRiD_264.jpg', 'IDRiD_265.jpg', 'IDRiD_266.jpg', 'IDRiD_267.jpg', 'IDRiD_268.jpg', 'IDRiD_269.jpg', 'IDRiD_270.jpg', 'IDRiD_271.jpg', 'IDRiD_272.jpg', 'IDRiD_273.jpg', 'IDRiD_274.jpg', 'IDRiD_275.jpg', 'IDRiD_276.jpg', 'IDRiD_277.jpg', 'IDRiD_278.jpg', 'IDRiD_279.jpg', 'IDRiD_280.jpg', 'IDRiD_281.jpg', 'IDRiD_282.jpg', 'IDRiD_283.jpg', 'IDRiD_284.jpg', 'IDRiD_285.jpg', 'IDRiD_286.jpg', 'IDRiD_287.jpg', 'IDRiD_288.jpg', 'IDRiD_289.jpg', 'IDRiD_290.jpg', 'IDRiD_291.jpg', 'IDRiD_292.jpg', 'IDRiD_293.jpg', 'IDRiD_294.jpg', 'IDRiD_295.jpg', 'IDRiD_296.jpg', 'IDRiD_297.jpg', 'IDRiD_298.jpg', 'IDRiD_299.jpg', 'IDRiD_300.jpg', 'IDRiD_301.jpg', 'IDRiD_302.jpg', 'IDRiD_303.jpg', 'IDRiD_304.jpg', 'IDRiD_305.jpg', 'IDRiD_306.jpg', 'IDRiD_307.jpg', 'IDRiD_308.jpg', 'IDRiD_309.jpg', 'IDRiD_310.jpg', 'IDRiD_311.jpg', 'IDRiD_312.jpg', 'IDRiD_313.jpg', 'IDRiD_314.jpg', 'IDRiD_315.jpg', 'IDRiD_316.jpg', 'IDRiD_317.jpg', 'IDRiD_318.jpg', 'IDRiD_319.jpg', 'IDRiD_320.jpg', 'IDRiD_321.jpg', 'IDRiD_322.jpg', 'IDRiD_323.jpg', 'IDRiD_324.jpg', 'IDRiD_325.jpg', 'IDRiD_326.jpg', 'IDRiD_327.jpg', 'IDRiD_328.jpg', 'IDRiD_329.jpg', 'IDRiD_330.jpg', 'IDRiD_331.jpg', 'IDRiD_332.jpg', 'IDRiD_333.jpg', 'IDRiD_334.jpg', 'IDRiD_335.jpg', 'IDRiD_336.jpg', 'IDRiD_337.jpg', 'IDRiD_338.jpg', 'IDRiD_339.jpg', 'IDRiD_340.jpg', 'IDRiD_341.jpg', 'IDRiD_342.jpg', 'IDRiD_343.jpg', 'IDRiD_344.jpg', 'IDRiD_345.jpg', 'IDRiD_346.jpg', 'IDRiD_347.jpg', 'IDRiD_348.jpg', 'IDRiD_349.jpg', 'IDRiD_350.jpg', 'IDRiD_351.jpg', 'IDRiD_352.jpg', 'IDRiD_353.jpg', 'IDRiD_354.jpg', 'IDRiD_355.jpg', 'IDRiD_356.jpg', 'IDRiD_357.jpg', 'IDRiD_358.jpg', 'IDRiD_359.jpg', 'IDRiD_360.jpg', 'IDRiD_361.jpg', 'IDRiD_362.jpg', 'IDRiD_363.jpg', 'IDRiD_364.jpg', 'IDRiD_365.jpg', 'IDRiD_366.jpg', 'IDRiD_367.jpg', 'IDRiD_368.jpg', 'IDRiD_369.jpg', 'IDRiD_370.jpg', 'IDRiD_371.jpg', 'IDRiD_372.jpg', 'IDRiD_373.jpg', 'IDRiD_374.jpg', 'IDRiD_375.jpg', 'IDRiD_376.jpg', 'IDRiD_377.jpg', 'IDRiD_378.jpg', 'IDRiD_379.jpg', 'IDRiD_380.jpg', 'IDRiD_381.jpg', 'IDRiD_382.jpg', 'IDRiD_383.jpg', 'IDRiD_384.jpg', 'IDRiD_385.jpg', 'IDRiD_386.jpg', 'IDRiD_387.jpg', 'IDRiD_388.jpg', 'IDRiD_389.jpg', 'IDRiD_390.jpg', 'IDRiD_391.jpg', 'IDRiD_392.jpg', 'IDRiD_393.jpg', 'IDRiD_394.jpg', 'IDRiD_395.jpg', 'IDRiD_396.jpg', 'IDRiD_397.jpg', 'IDRiD_398.jpg', 'IDRiD_399.jpg', 'IDRiD_400.jpg', 'IDRiD_401.jpg', 'IDRiD_402.jpg', 'IDRiD_403.jpg', 'IDRiD_404.jpg', 'IDRiD_405.jpg', 'IDRiD_406.jpg', 'IDRiD_407.jpg', 'IDRiD_408.jpg', 'IDRiD_409.jpg', 'IDRiD_410.jpg', 'IDRiD_411.jpg', 'IDRiD_412.jpg', 'IDRiD_413.jpg']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEjfLxfS7UKx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\"\"\"define data augmentation functions\"\"\"\n",
        "\n",
        "\"\"\"flip a image upsidedown\"\"\"\n",
        "@tf.function\n",
        "def flip1(img):\n",
        "    img_flipped = tf.image.random_flip_up_down(img)\n",
        "    img = tf.cast(img_flipped, tf.float32) / 255.0  # normalise the image after flippig\n",
        "    return img\n",
        "\n",
        "\n",
        "\"\"\"flip a image left and right\"\"\"\n",
        "def flip2(img):\n",
        "    img_flipped = tf.image.random_flip_left_right(img)\n",
        "    img = tf.cast(img_flipped, tf.float32) / 255.0    # normalise the image after flippig\n",
        "    return img\n",
        "\n",
        "\n",
        "\"\"\"rotate a image in an random selected angle from 0 to 20\"\"\"\n",
        "def rotate(img):\n",
        "    angles = tf.random.uniform([], minval=0, maxval=20, dtype=tf.dtypes.float32)\n",
        "    img = tfa.image.rotate(img, angles, interpolation='NEAREST', name=None)\n",
        "    img = tf.cast(img, tf.float32) / 255.0\n",
        "    return img\n",
        "\n",
        "\n",
        "\"\"\"zoom a image \"\"\"\n",
        "def zoom(img):\n",
        "    scales = list(np.arange(0.8, 1.0, 0.01))  \n",
        "    boxes = np.zeros((len(scales), 4))\n",
        "\n",
        "    for i, scale in enumerate(scales):\n",
        "        x1 = y1 = 0.5 - (0.5 * scale)\n",
        "        x2 = y2 = 0.5 + (0.5 * scale)\n",
        "        boxes[i] = [x1, y1, x2, y2]\n",
        "\n",
        "    \"\"\"Create different crops for an image and return a random crop\"\"\"\n",
        "    def random_crop(img):\n",
        "        crops = tf.image.crop_and_resize([img], boxes=boxes, box_indices=np.zeros(len(scales)), crop_size=(256,256))\n",
        "        return crops[tf.random.uniform(shape=[], minval=0, maxval=len(scales), dtype=tf.int32)]  \n",
        "\n",
        "    choice = tf.random.uniform(shape=[], minval=0., maxval=1., dtype=tf.float32)\n",
        "\n",
        "    \"\"\"Only apply cropping 50% of the time\"\"\"\n",
        "    return tf.cond(choice < 0.5, lambda: img, lambda: random_crop(img))\n",
        "\n",
        "\n",
        "\"\"\"Rotate a image 90 degree\"\"\"\n",
        "def rot90(img):\n",
        "    img = tf.image.rot90(img, tf.random.uniform(shape=[], minval=0, maxval=4, dtype=tf.int32))\n",
        "    img = tf.cast(img, tf.float32) / 255.0\n",
        "    return img\n",
        "\n",
        "\n",
        "\"\"\"Create summary of all augmentations\"\"\"\n",
        "augmentations = [flip1, flip2, rot90, zoom]\n",
        "\n",
        "\n",
        "\"\"\"The function get one filename and returns an image\"\"\"\n",
        "def parse_function(files):\n",
        "    image_string = tf.io.read_file(files)\n",
        "    image_decoded = tf.io.decode_jpeg(image_string)\n",
        "    image_resized = tf.image.resize_with_pad(image_decoded, 256, 256)\n",
        "    return image_resized\n",
        "\n",
        "\n",
        "\"\"\"Build training dataset and apply online augmentation\"\"\"\n",
        "def build_train_ds(files, labels, batchsize):\n",
        "    ds_x = tf.data.Dataset.from_tensor_slices(files)\n",
        "    ds_x = ds_x.map(parse_function, N_parallel_iteration)\n",
        "    # Apply the augmentation, run 4 jobs in parallel.\n",
        "    # Apply to the training dataset\n",
        "    for f in augmentations:   \n",
        "      ds_x = ds_x.map(f)\n",
        "    # Make sure that the values are still in [0, 1]\n",
        "    ds_x = ds_x.map(lambda x: tf.clip_by_value(x, 0, 1))\n",
        "    ds_y = tf.data.Dataset.from_tensor_slices(labels)\n",
        "    ds = tf.data.Dataset.zip((ds_x,ds_y))\n",
        "    ds = ds.shuffle(380).batch(batchsize).repeat(-1).prefetch(N_prefetch)\n",
        "    return ds\n",
        "\n",
        "\n",
        "\"\"\"Build validation dataset without augmentation\"\"\"\n",
        "def build_val_ds(files, labels, batchsize):\n",
        "    ds_x = tf.data.Dataset.from_tensor_slices(files)\n",
        "    ds_x = ds_x.map(parse_function)\n",
        "    ds_y = tf.data.Dataset.from_tensor_slices(labels)\n",
        "    ds = tf.data.Dataset.zip((ds_x,ds_y))\n",
        "    ds = ds.shuffle(20).batch(batchsize).prefetch(N_prefetch)\n",
        "    return ds\n",
        "\n",
        "\n",
        "\"\"\"Shuffle the data before split them to training and validation sets\"\"\"\n",
        "shuffle_idx = np.arange(0, N_samples)\n",
        "np.random.shuffle(shuffle_idx) \n",
        "files = [files[i] for i in shuffle_idx]       # shuffle index of filenames, shortens time\n",
        "labels = [labels[i] for i in shuffle_idx]\n",
        "\n",
        "\n",
        "\"\"\"Build total dataset(training set and validtion set)\"\"\"\n",
        "train_ds = build_train_ds(files[0:N_trainingsamples], labels[0:N_trainingsamples], batchsize)\n",
        "val_ds = build_val_ds(files[N_trainingsamples:N_samples], labels[N_trainingsamples:N_samples], batchsize)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gg_glKksLQEi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\"\"\"build test dataset of images\"\"\"\n",
        "\n",
        "def load_testfile_names():\n",
        "    files = glob.glob(\n",
        "        \"/content/gdrive/My Drive/Masterlab/b. Testing Set/*.jpg\")\n",
        "    return files\n",
        "\n",
        "\n",
        "test_img_files = load_testfile_names()\n",
        "\n",
        "\n",
        "def build_test_img_ds(input_file):\n",
        "  img_list_test = []\n",
        "  for file in sorted(input_file):\n",
        "    image_string = tf.io.read_file(file)\n",
        "    image_decoded = tf.io.decode_image(image_string)\n",
        "    image_resized = tf.image.resize_with_pad(image_decoded, 256, 256)\n",
        "    img = tf.cast(image_resized, tf.float32) / 255.0\n",
        "    img_list_test.append(img)\n",
        "\n",
        "    img_tensor_test = tf.convert_to_tensor(img_list_test, dtype=tf.float32)\n",
        "    img_test_ds = tf.data.Dataset.from_tensor_slices(img_tensor_test)\n",
        "\n",
        "  return img_test_ds\n",
        "\n",
        "\n",
        "# build dataset of labels(testing set)\n",
        "\n",
        "csv_path_test = \"/content/gdrive/My Drive/Masterlab/b. IDRiD_Disease Grading_Testing Labels.csv\"\n",
        "files_csv_test = pd.read_csv(csv_path_test, usecols=[1])\n",
        "\n",
        "\n",
        "\"\"\"build total dataset(testing set)\"\"\"\n",
        "def build_test_ds(input_file, batchsize):\n",
        "  img_ds = build_test_img_ds(input_file)\n",
        "\n",
        "  labels_test = create_label(files_csv_test, N_testsamples)   \n",
        "  label_ds_test = tf.data.Dataset.from_tensor_slices(labels_test)\n",
        "  ds = tf.data.Dataset.zip((img_ds, label_ds_test)).batch(batchsize)\n",
        "  return ds\n",
        "\n",
        "  \n",
        "test_ds = build_test_ds(test_img_files, batchsize)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}